import os
import torch
import torch.nn as nn
import torchvision.models as models
from time import sleep, time


def run_stress_test():
    # -------------------------------------------------------------------
    # 1. é…ç½®å’Œæ£€æŸ¥ç¯å¢ƒ (Setup & Sanity Checks)
    # -------------------------------------------------------------------

    # å‡è®¾ä½¿ç”¨0å·å’Œ1å·GPU
    gpu_ids_str = '0,1'
    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_ids_str

    try:
        device_ids = [int(id) for id in gpu_ids_str.split(',')]
    except ValueError:
        print(f"é”™è¯¯: GPU ID '{gpu_ids_str}' æ ¼å¼ä¸æ­£ç¡®ã€‚")
        return

    if not torch.cuda.is_available():
        print("é”™è¯¯: PyTorch æœªæ£€æµ‹åˆ°å¯ç”¨çš„CUDAç¯å¢ƒã€‚")
        return

    num_gpus = torch.cuda.device_count()
    if num_gpus < len(device_ids):
        print(f"é”™è¯¯: æ£€æµ‹åˆ°çš„GPUæ•°é‡ ({num_gpus}) å°‘äºæ‚¨æŒ‡å®šçš„æ•°é‡ ({len(device_ids)})ã€‚")
        return

    print("===================== 4090 å¤šå¡å‹åŠ›æµ‹è¯• =====================")
    print(f"âœ… PyTorch CUDA å¯ç”¨, æ£€æµ‹åˆ° {num_gpus} ä¸ªGPUã€‚")
    print(f"âœ… æœ¬æ¬¡æµ‹è¯•å°†ä½¿ç”¨ GPU: {device_ids}")
    for i in device_ids:
        print(f"  - GPU {i}: {torch.cuda.get_device_name(i)}")
    print("==============================================================")

    # -------------------------------------------------------------------
    # 2. åˆ›å»ºä¸€ä¸ªæ›´å¤æ‚çš„æ¨¡å‹ (Instantiate a Complex Model)
    # -------------------------------------------------------------------

    # ä½¿ç”¨ç»å…¸çš„ResNet-50æ¨¡å‹ï¼Œå®ƒæ¯”ä¹‹å‰çš„ç®€å•CNNå¤æ‚å¾—å¤š
    # weights=None ç¡®ä¿ä¸ä¼šè§¦å‘ä»»ä½•ç½‘ç»œä¸‹è½½
    print("\næ­£åœ¨åˆ›å»º ResNet-50 æ¨¡å‹...")
    model = models.resnet50(weights=None)

    # å°†æ¨¡å‹ä¸»ä½“å…ˆæ”¾åˆ°ä¸»GPUä¸Š (ä¾‹å¦‚: device 0)
    primary_device = f'cuda:{device_ids[0]}'
    model.to(primary_device)

    # --- æ ¸å¿ƒæ­¥éª¤: ä½¿ç”¨DataParallelå°†æ¨¡å‹å°è£…ä»¥æ”¯æŒå¤šå¡ ---
    model = nn.DataParallel(model, device_ids=device_ids)
    print("âœ… æ¨¡å‹å·²æˆåŠŸä½¿ç”¨ torch.nn.DataParallel è¿›è¡Œå°è£…ã€‚")

    # -------------------------------------------------------------------
    # 3. æ¨¡æ‹Ÿé«˜è´Ÿè½½è®­ç»ƒ (Simulate High-Load Training)
    # -------------------------------------------------------------------

    # é’ˆå¯¹24Gæ˜¾å­˜ï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®éå¸¸å¤§çš„Batch Sizeæ¥å¢åŠ è´Ÿè½½
    # ä¾‹å¦‚ï¼Œæ¯ä¸ªGPUå¤„ç†256ä¸ªæ ·æœ¬
    batch_size_per_gpu = 256
    total_batch_size = batch_size_per_gpu * len(device_ids)
    img_size = 224  # ResNetæ ‡å‡†è¾“å…¥å°ºå¯¸
    num_test_steps = 20

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

    print("\nğŸš€ å¼€å§‹æ¨¡æ‹Ÿé«˜è´Ÿè½½è®­ç»ƒ...")
    print(f"   - æ¨¡å‹: ResNet-50")
    print(f"   - å›¾åƒå°ºå¯¸: {img_size}x{img_size}")
    print(f"   - æ¯ä¸ªGPUçš„æ‰¹é‡: {batch_size_per_gpu}")
    print(f"   - æ€»æ‰¹é‡å¤§å°: {total_batch_size}")
    print("\nğŸ”¥ è¯·ç«‹å³æ–°å¼€ä¸€ä¸ªç»ˆç«¯ï¼Œå¹¶è¿è¡Œ `watch -n 1 nvidia-smi` æ¥æŒç»­ç›‘æ§GPUçŠ¶æ€ã€‚")
    print("   æ‚¨åº”è¯¥ä¼šçœ‹åˆ°ä¸¤å¼ 4090çš„åŠŸè€—(Power)ã€æ˜¾å­˜(Memory)å’Œåˆ©ç”¨ç‡(GPU-Util)éƒ½æ˜¾è‘—ä¸Šå‡ã€‚")
    sleep(5)

    # é¢„çƒ­ä¸€æ¬¡CUDAï¼Œè®©è®¡æ—¶æ›´å‡†ç¡®
    print("\næ­£åœ¨è¿›è¡ŒCUDAé¢„çƒ­...")
    s = torch.cuda.Stream()
    s.wait_stream(torch.cuda.current_stream())
    torch.cuda.synchronize()
    print("é¢„çƒ­å®Œæˆï¼Œæµ‹è¯•å¼€å§‹ï¼")

    start_time = time()
    for i in range(num_test_steps):
        iter_start_time = time()

        # åŠ¨æ€åˆ›å»ºè¶³å¤Ÿå¤§çš„å‡çš„å›¾åƒå’Œæ ‡ç­¾ï¼Œå¹¶æ”¾åˆ°ä¸»GPUä¸Š
        fake_images = torch.randn(total_batch_size, 3, img_size, img_size, device=primary_device)
        fake_labels = torch.randint(0, 1000, (total_batch_size,), device=primary_device)

        # å‰å‘ä¼ æ’­ (DataParallelè‡ªåŠ¨åˆ‡åˆ†æ•°æ®åˆ°æ‰€æœ‰GPU)
        outputs = model(fake_images)

        # è®¡ç®—æŸå¤±
        loss = criterion(outputs, fake_labels)

        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ç­‰å¾…æ‰€æœ‰GPUæ ¸å¿ƒå®Œæˆå½“å‰æ‰¹æ¬¡è®¡ç®—
        torch.cuda.synchronize()
        iter_end_time = time()

        print(f"  [æµ‹è¯•æ­¥éª¤ {i + 1:02d}/{num_test_steps}] æŸå¤±å€¼: {loss.item():.4f} | "
              f"è€—æ—¶: {(iter_end_time - iter_start_time) * 1000:.2f} ms")

    end_time = time()
    total_time = end_time - start_time
    avg_time_per_step = total_time / num_test_steps

    print("\n==============================================================")
    print("ğŸ‰ å‹åŠ›æµ‹è¯•å®Œæˆï¼")
    print(f"æ€»è®¡ {num_test_steps} ä¸ªæ¨¡æ‹Ÿæ­¥éª¤è€—æ—¶ {total_time:.2f} ç§’ã€‚")
    print(f"å¹³å‡æ¯æ­¥è€—æ—¶: {avg_time_per_step * 1000:.2f} msã€‚")
    print("\n[æœ€ç»ˆç¡®è®¤]:")
    print("  - å¦‚æœåœ¨æµ‹è¯•ä¸­æ²¡æœ‰æŠ¥é”™ã€‚")
    print("  - å¹¶ä¸”æ‚¨åœ¨ `nvidia-smi` ä¸­è§‚å¯Ÿåˆ°ä¸¤å¼ 4090çš„åŠŸè€—ã€æ˜¾å­˜å’Œåˆ©ç”¨ç‡éƒ½è¾¾åˆ°äº†è¾ƒé«˜æ°´å¹³ã€‚")
    print("  ==> é‚£ä¹ˆæ‚¨çš„åŒ4090å¤šå¡ç¯å¢ƒé…ç½®éå¸¸æˆåŠŸï¼Œä¸”æ€§èƒ½æ­£å¸¸ï¼")
    print("==============================================================")


if __name__ == '__main__':
    # éœ€è¦torchvisionåŒ…ï¼Œå¦‚æœæ²¡å®‰è£…ï¼Œæç¤ºç”¨æˆ·å®‰è£…
    try:
        import torchvision
    except ImportError:
        print("é”™è¯¯: æœªæ‰¾åˆ° torchvision åŒ…ã€‚è¯·å…ˆå®‰è£…: pip install torchvision")
    else:
        run_stress_test()